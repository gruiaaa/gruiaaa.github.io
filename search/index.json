[{"content":"关于使用深度学习提高模型精确率至95% 可行性分析 [toc]\n结论：在此数据集(样本正反比例)下不行\n结论依据从如下两方面讨论\n手动搭建全连接网络与卷积神经网络对比分析 Autogluon框架自动调参结果分析 样本比例对模型精度影响分析 手动搭建全连接网络与卷积神经网络对比分析 全连接网络 人工神经网络中的神经元为全连接，是线性的叠加，从理论上来说，达到一定程度后，全连接层数与模型精确度关系不大。进行实验验证，结果确实如此。\n一、两层网络测试结果\n两层网络结构 模型训练精确率保持不变，为0.86左右 二、多层网络测试结果\n多层网络结构 精确率同样保持在0.86附近 从两个结果对比可知，全连接层数对精确率没有影响。同时从训练过程中的loss值可以看出模型已经达到了拟合状态，无法提高。\n尝试用卷积神经网络。\n卷积神经网络分析 由于数据为一维向量，采取CONV1D进行卷积，设计了8层卷积层与2层全连接层。网络结构及实验结果如下。\n卷积网络结构 实验结果显示精确率同样保持在0.86左右 经过两种网络实验后分析可知，在此数据集下，使用神经网络进行预测精确度只能保持在0.86左右，所以理论上达不到95%的精确率。\nAutogluon框架自动调参结果分析 Autogluon是一个自动机器学习的库，可以实现自动调参、模型选择。主要应用场景有\n机器学习：实现机器学习中的预测问题；\n图像分类：识别图像中的主要对象；\n对象检测：借助图像中的边界框检测多个对象；\n文本分类：基于文本内容做出预测。\n使用此框架对游戏数据最终样本.csv进行分析。代码及运行情况如下。\n代码 运行情况 结果存为result.txt。结果如下表\nindex model score_test score_val pred_time_test pred_time_val fit_time pred_time_test_marginal pred_time_val_marginal fit_time_marginal stack_level can_infer fit_order 0 XGBoost 0.878917 0.877828 0.172556 0.015625 3.227387 0.172556 0.015625 3.227387 1 True 11 1 CatBoost 0.878668 0.880737 0.067747 0.015619 10.656250 0.067747 0.015619 10.656250 1 True 7 2 LightGBM 0.878615 0.877505 0.185287 0.015621 1.499961 0.185287 0.015621 1.499961 1 True 4 3 ExtraTreesGini 0.877778 0.892372 2.339057 0.031242 10.179940 2.339057 0.031242 10.179940 1 True 8 4 ExtraTreesEntr 0.877770 0.895281 2.582575 0.046864 12.298146 2.582575 0.046864 12.298146 1 True 9 5 RandomForestEntr 0.877521 0.928895 2.755487 0.046864 34.587802 2.755487 0.046864 34.587802 1 True 6 6 WeightedEnsemble_L2 0.877476 0.930511 5.226238 0.109824 59.683161 0.015620 0.015586 1.704907 2 True 14 7 RandomForestGini 0.876443 0.925986 2.455130 0.047374 23.390453 2.455130 0.047374 23.390453 1 True 5 8 NeuralNetFastAI 0.876390 0.877828 1.069277 0.046863 210.108015 1.069277 0.046863 210.108015 1 True 10 9 NeuralNetTorch 0.871841 0.885262 0.454326 0.027925 714.680279 0.454326 0.027925 714.680279 1 True 12 10 LightGBMLarge 0.870355 0.866839 0.047337 0.017444 0.808270 0.047337 0.017444 0.808270 1 True 13 11 LightGBMXT 0.870355 0.866839 0.140592 0.000000 1.562100 0.140592 0.000000 1.562100 1 True 3 12 KNeighborsUnif 0.841457 0.850679 2.043974 0.078107 4.479971 2.043974 0.078107 4.479971 1 True 1 13 KNeighborsDist 0.825307 0.855850 2.058742 0.046836 1.718632 2.058742 0.046836 1.718632 1 True 2 表中加粗列为验证集精确率，可知精确率在90%以上的共有三个模型，分别为RandomForestEntr、WeightedEnsemble_L2、RandomForestGini,精确率在93%附件，三者精确率都没有超过95%。\n其中两个神经网络及其精确率为 NeuralNetFastAI - 0.877828，NeuralNetTorch - 0.885262 ,与手动搭建神经网络结果相差不大。\n结果分析 通过两方面对比分析，在此数据集上，使用深度学习构建神经网络来做二分类预测的效果并不最佳，随机森林效果更好，使用神经网络并不能提升模型精确率至95%以上。\n样本比例对模型精度影响分析 在进行分析时，统计发现样本比例不均，比例约为1：6，情况如下。\n考虑到样本比例不均是否会对模型精确度造成影响，于是选择部分数据进行分析。\n方式1 比例变且样本量变 调整样本数量为25W，其中正例58k，比例约为1：4，运行情况如下图左\n调整样本数量为30W，其中正例58k，比例约为1：5，运行情况如下图右\nTotal：25W Ratio：1：4 Accuracy：0.76 Total：30W Ratio：1：5 Accuracy：0.80 此对比说明样本数量对模型精确度仍有影响，所以合理推测，使神经网络模型精确率在90%以下的原因之一是样本数据量不够。\n方式2 比例变但样本量不变 实验结果有点意外，结果全连接层构成的神经网络在精度上与样本比例有关，结果如下\n运行结果截图 样本量、比率、精确率 Total:100000 Ratio: 1:1 Accuracy: 0.49 Total:200000 Ratio: 1:5 Accuracy: 0.83 Total:200000 Ratio: 1:10 Accuracy: 0.90 Total:200000 Ratio: 1:15 Accuracy: 0.93 从结果分析，样本比率对全连接神经网络的精确率影响较大。\n当比例达到1：20时，精确率可达95%。考虑此种情况出现原因为比例过于不协调导致训练测试过拟合。\n思考可能原因是自己手动搭建的模型不合理，于是使用Autoloun 对不同比例进行测试。\n固定样本数量为20W，比例为1:50时，结果如下表\nindex model score_test score_val pred_time_test pred_time_val fit_time pred_time_test_marginal pred_time_val_marginal fit_time_marginal stack_level can_infer fit_order 0 LightGBM 1.000000 0.972011 0.015057 0.006982 0.392086 0.015057 0.006982 0.392086 1 True 4 1 LightGBMXT 1.000000 0.972011 0.017099 0.006028 1.578239 0.017099 0.006028 1.578239 1 True 3 2 LightGBMLarge 1.000000 0.972011 0.029131 0.005984 0.539557 0.029131 0.005984 0.539557 1 True 13 3 CatBoost 1.000000 0.972011 0.062832 0.012073 6.339178 0.062832 0.012073 6.339178 1 True 7 4 NeuralNetTorch 1.000000 0.972011 0.228472 0.021114 55.337163 0.228472 0.021114 55.337163 1 True 12 5 NeuralNetFastAI 1.000000 0.972011 1.069749 0.206472 118.090308 1.069749 0.206472 118.090308 1 True 10 6 ExtraTreesGini 0.999817 0.973611 1.634300 0.056363 5.724418 1.634300 0.056363 5.724418 1 True 8 7 ExtraTreesEntr 0.999783 0.974010 1.750266 0.058498 6.254190 1.750266 0.058498 6.254190 1 True 9 8 RandomForestEntr 0.999783 0.973611 2.301649 0.090802 36.538411 2.301649 0.090802 36.538411 1 True 6 9 RandomForestGini 0.999783 0.974010 2.994298 0.104868 47.326895 2.994298 0.104868 47.326895 1 True 5 10 WeightedEnsemble_L2 0.999783 0.974010 3.001316 0.109909 48.754825 0.007018 0.005041 1.427929 2 True 14 11 XGBoost 0.999717 0.972811 0.118953 0.022081 1.722954 0.118953 0.022081 1.722954 1 True 11 12 KNeighborsUnif 0.999000 0.971212 0.932512 0.061589 5.656096 0.932512 0.061589 5.656096 1 True 1 13 KNeighborsDist 0.994417 0.966413 0.637997 0.059638 0.835501 0.637997 0.059638 0.835501 1 True 2 固定样本数量为20W，比例为1:10时，结果如下表\nindex model score_test score_val pred_time_test pred_time_val fit_time pred_time_test_marginal pred_time_val_marginal fit_time_marginal stack_level can_infer fit_order 0 LightGBMLarge 1.000000 0.870052 0.017952 0.005975 0.684027 0.017952 0.005975 0.684027 1 True 13 1 LightGBMXT 1.000000 0.870052 0.041129 0.006102 1.060362 0.041129 0.006102 1.060362 1 True 3 2 NeuralNetFastAI 0.993217 0.878848 1.309562 0.278254 186.526137 1.309562 0.278254 186.526137 1 True 10 3 NeuralNetTorch 0.983817 0.882447 0.262298 0.023937 218.103173 0.262298 0.023937 218.103173 1 True 12 4 XGBoost 0.982617 0.881647 0.108709 0.024933 2.098191 0.108709 0.024933 2.098191 1 True 11 5 LightGBM 0.982283 0.880448 0.189728 0.011000 1.127144 0.189728 0.011000 1.127144 1 True 4 6 ExtraTreesEntr 0.981550 0.893243 2.669475 0.078789 5.102355 2.669475 0.078789 5.102355 1 True 9 7 CatBoost 0.979800 0.878449 0.107712 0.005984 4.745862 0.107712 0.005984 4.745862 1 True 7 8 ExtraTreesGini 0.979467 0.892443 2.306153 0.038895 4.267919 2.306153 0.038895 4.267919 1 True 8 9 RandomForestEntr 0.979250 0.908836 2.660665 0.054853 22.674685 2.660665 0.054853 22.674685 1 True 6 10 WeightedEnsemble_L2 0.978783 0.911236 5.604127 0.121389 48.126948 0.021942 0.004987 1.584814 2 True 14 11 RandomForestGini 0.977450 0.905638 2.813807 0.055566 19.121587 2.813807 0.055566 19.121587 1 True 5 12 KNeighborsUnif 0.971167 0.854458 1.097421 0.051967 4.141845 1.097421 0.051967 4.141845 1 True 1 13 KNeighborsDist 0.948333 0.858057 0.907242 0.031711 0.695398 0.907242 0.031711 0.695398 1 True 2 固定样本数量为20W，比例为1:20时，结果如下表\nindex model score_test score_val pred_time_test pred_time_val fit_time pred_time_test_marginal pred_time_val_marginal fit_time_marginal stack_level can_infer fit_order 0 LightGBM 1.000000 0.932027 0.047871 0.006981 0.500660 0.047871 0.006981 0.500660 1 True 4 1 LightGBMXT 1.000000 0.932027 0.058845 0.006981 1.115775 0.058845 0.006981 1.115775 1 True 3 2 LightGBMLarge 1.000000 0.932027 0.186760 0.007979 0.722069 0.186760 0.007979 0.722069 1 True 13 3 CatBoost 1.000000 0.932027 0.516031 0.018466 6.661329 0.516031 0.018466 6.661329 1 True 7 4 XGBoost 0.998950 0.934426 0.357095 0.029920 2.227147 0.357095 0.029920 2.227147 1 True 11 5 NeuralNetTorch 0.997450 0.935226 0.512716 0.024933 232.269407 0.512716 0.024933 232.269407 1 True 12 6 ExtraTreesEntr 0.995983 0.934826 2.193199 0.034345 4.701628 2.193199 0.034345 4.701628 1 True 9 7 RandomForestGini 0.995917 0.938425 2.768432 0.061291 30.934240 2.768432 0.061291 30.934240 1 True 5 8 WeightedEnsemble_L2 0.995850 0.939624 7.259789 0.099869 42.106585 0.063156 0.007063 4.319468 2 True 14 9 ExtraTreesGini 0.995833 0.935226 2.036377 0.035958 4.902179 2.036377 0.035958 4.902179 1 True 8 10 RandomForestEntr 0.995700 0.939224 5.160256 0.056848 32.884938 5.160256 0.056848 32.884938 1 True 6 11 NeuralNetFastAI 0.995450 0.933227 2.326203 0.145610 125.404017 2.326203 0.145610 125.404017 1 True 10 12 KNeighborsUnif 0.993500 0.926429 1.109035 0.062525 4.202966 1.109035 0.062525 4.202966 1 True 1 13 KNeighborsDist 0.981233 0.919632 1.148926 0.045874 0.774013 1.148926 0.045874 0.774013 1 True 2 固定样本数量为40W，比例为1:20时，结果如下表\nindex model score_test score_val pred_time_test pred_time_val fit_time pred_time_test_marginal pred_time_val_marginal fit_time_marginal stack_level can_infer fit_order 0 LightGBMLarge 1.000000 0.932143 0.180578 0.009974 0.957314 0.180578 0.009974 0.957314 1 True 13 1 LightGBMXT 1.000000 0.932143 0.189680 0.006024 1.684334 0.189680 0.006024 1.684334 1 True 3 2 LightGBM 1.000000 0.932143 0.208429 0.006981 0.767455 0.208429 0.006981 0.767455 1 True 4 3 CatBoost 0.999983 0.932143 0.214847 0.010021 6.541359 0.214847 0.010021 6.541359 1 True 7 4 NeuralNetFastAI 0.999133 0.933214 1.084537 0.160347 201.239708 1.084537 0.160347 201.239708 1 True 10 5 ExtraTreesEntr 0.999058 0.937143 2.776712 0.048870 12.369458 2.776712 0.048870 12.369458 1 True 9 6 NeuralNetTorch 0.998733 0.933571 0.483066 0.026928 299.167305 0.483066 0.026928 299.167305 1 True 12 7 XGBoost 0.998483 0.933571 0.158724 0.011024 2.725587 0.158724 0.011024 2.725587 1 True 11 8 ExtraTreesGini 0.998158 0.934286 2.386469 0.082176 9.599849 2.386469 0.082176 9.599849 1 True 8 9 RandomForestEntr 0.995817 0.945714 3.373599 0.047966 41.806864 3.373599 0.047966 41.806864 1 True 6 10 WeightedEnsemble_L2 0.995683 0.946429 6.271379 0.103337 81.883340 0.003990 0.006025 1.588467 2 True 14 11 RandomForestGini 0.995375 0.943929 2.893790 0.049345 38.488010 2.893790 0.049345 38.488010 1 True 5 12 KNeighborsUnif 0.992700 0.928929 2.076654 0.086878 7.806824 2.076654 0.086878 7.806824 1 True 1 13 KNeighborsDist 0.983300 0.925357 2.044742 0.036018 1.930425 2.044742 0.036018 1.930425 1 True 2 固定样本数量为5W，比例为1:20时，结果如下表\nindex model score_test score_val pred_time_test pred_time_val fit_time pred_time_test_marginal pred_time_val_marginal fit_time_marginal stack_level can_infer fit_order 0 NeuralNetTorch 0.999533 0.9320 0.079349 0.021101 22.859513 0.079349 0.021101 22.859513 1 True 12 1 NeuralNetFastAI 0.999467 0.9328 0.237272 0.052859 46.644695 0.237272 0.052859 46.644695 1 True 10 2 LightGBMXT 0.999133 0.9360 0.098287 0.029206 1.986610 0.098287 0.029206 1.986610 1 True 3 3 XGBoost 0.998533 0.9352 0.018100 0.009973 1.708424 0.018100 0.009973 1.708424 1 True 11 4 CatBoost 0.998333 0.9356 0.053169 0.008051 8.302693 0.053169 0.008051 8.302693 1 True 7 5 ExtraTreesGini 0.997200 0.9336 0.599552 0.129022 2.926774 0.599552 0.129022 2.926774 1 True 8 6 ExtraTreesEntr 0.997133 0.9344 0.732795 0.096238 2.703793 0.732795 0.096238 2.703793 1 True 9 7 LightGBM 0.996867 0.9360 0.091236 0.020051 0.962924 0.091236 0.020051 0.962924 1 True 4 8 RandomForestEntr 0.996600 0.9360 0.421602 0.096128 13.725857 0.421602 0.096128 13.725857 1 True 6 9 RandomForestGini 0.996533 0.9344 0.435460 0.218416 12.675858 0.435460 0.218416 12.675858 1 True 5 10 LightGBMLarge 0.995533 0.9380 0.202538 0.036901 2.375687 0.202538 0.036901 2.375687 1 True 13 11 WeightedEnsemble_L2 0.995533 0.9380 0.209553 0.040954 3.814684 0.007015 0.004053 1.438997 2 True 14 12 KNeighborsUnif 0.993600 0.9264 0.076322 0.054854 4.625237 0.076322 0.054854 4.625237 1 True 1 13 KNeighborsDist 0.979933 0.9136 0.085319 0.023937 0.146608 0.085319 0.023937 0.146608 1 True 2 结果证明，二分类中样本正反比率对模型精确度有较大影响。\n总结 本次实验主要是对游戏数据最终样本.csv进行二分类问题研究，实验结论如下：\n在此样本下，随机森林精确率最高，神经网络表现不如随机森林。 卷积神经网络和全连接神经网络在此二分类问题上的表现差别不大。 二分类问题中，在同一样本比例下，样本数量对结果准确率有一点影响，在1%以内，几乎没有。 二分类问题中样本比率对模型精确率影响较大，通过修改样本比率可使精确率达到95%甚至更高。 ","date":"2023-03-04","permalink":"https://gruiaaa.github.io/post/%E5%8F%AF%E8%A1%8C%E6%80%A7%E5%88%86%E6%9E%90/","tags":null,"title":"可行性分析"},{"content":"link：【Jeff】22天练出腹肌（2023年版）\n1.下腹卷尖刺\n10个一组，间隔休息10s\n2.坐姿提膝转腿\n45s为一组，间隔休息10s，力竭\n3.爆发起身\n10个一组，间隔休息10s，力竭\n4.坐姿高位转体\n5.反向画7（俯身提膝+转体）\n力竭\n6.日出仰卧起坐\n45s一组\n","date":"2023-02-28","permalink":"https://gruiaaa.github.io/post/jeff/","tags":null,"title":"Jeff"},{"content":"第一章 算法概述 常见的时间复杂度，及其增长速度比较 时间复杂度反映的是随着问题规模的变大，计算所需的时间的增长速度，与系数的多少关系不大\n渐进符号 O 大O 渐进上界 存在两个正常数c和n0，使得当n≥n0时，有f(n)≤cg(n)，则记做f(n) = O(g(n))\n示例\nf(n) = 2n + 3 = O(n)\n当n≥3时，2n+3≤3n，\n所以，可选c = 3，n0 = 3。对于n≥n0，f(n) = 2n + 3≤3n，\n所以， f(n) = O(n) 。这意味着，当n≥3时，该程序步不会超过3n。\nfn的阶不低于On\nΩ 欧米噶 渐近下界 存在两个正常数 c和n0，使得当n≥n0时，有f(n) ≥c g(n)，则记做f(n) = Ω (g(n))\nfn的阶不高于On\n例1-5 f(n) = 2n + 3 =Ω(n)\n对所有n，2n+3≥2n，可选c = 2，n0=0。\n对于n≥n0，f(n) = 2n+3≥2n，所以，f(n) = Ω(n)，即2n + 3∈Ω(n)。\n例1-6 f(n) = 10n2 + 4n + 2 = Ω(n2)\n对所有n，10n2 + 4n + 2≥10n2，可选c = 10，n0 = 0。\n对于n≥n0，f(n) = 10n2 + 4n + 2≥10n2，所以，f(n) =Ω(n^2)。\nθ Theta紧渐近界 存在正常数c1，c2和n0，使得当n≥n0时，有c1 g(n)≤f(n)≤c2 g(n)，则记做f(n) = θ(g(n))\nfn 、On 同阶\n例1-7\nf(n) = 2n + 3 = θ(n)\n例1-8\nf(n) = 10n2 + 4n + 2 = θ(n^2)\n算法按时间复杂度分类 分为\n多项式时间算法\n指数时间算法\n复杂度示意图\n常见排序算法时间复杂度 常用的排序算法的时间复杂度和空间复杂度\n排序法 最差时间分析 平均时间复杂度 稳定度 空间复杂度 冒泡排序 O(n2) O(n2) 稳定 O(1) 插入排序 O(n2) O(n2) 稳定 O(1) 选择排序 O(n2) O(n2) 不稳定 O(1) 二叉树排序 O(n2) O(n*log2n) 不一定 O(n) 快速排序 O(n2) O(n*log2n) 不稳定 O(log2n)~O(n) 堆排序 O(n*log2n) O(n*log2n) 不稳定 O(1) 希尔排序 O(n^(1.3-2)) O(n^(1.3-2)) 不稳定 O(1) 常见查找算法时间复杂度 查找 查找条件 平均时间复杂度 算法描述 顺序查找 无序或有序队列 O(n) 按顺序比较每个元素，直到找到关键字为止 二分查找（折半查找） 有序数组 O(logn) 查找过程从数组的中间元素开始，如果中间元素正好是要查找的元素，则搜素过程结束；如果某一特定元素大于或者小于中间元素，则在数组大于或小于中间元素的那一半中查找，而且跟开始一样从中间元素开始比较。　如果在某一步骤数组为空，则代表找不到。 二叉排序树查找 二叉排序树 O(logn) 在二叉查找树b中查找x的过程为：1. 若b是空树，则搜索失败2. 若x等于b的根节点的数据域之值，则查找成功；3. 若x小于b的根节点的数据域之值，则搜索左子树4. 查找右子树。 哈希表法（散列表） 先创建哈希表（散列表） O(1) 根据键值方式(Key value)进行查找，通过散列函数，定位数据元素。 分块查找 无序或有序队列 O(logn) 将n个数据元素\u0026quot;按块有序\u0026quot;划分为m块（m ≤ n）。每一块中的结点不必有序，但块与块之间必须\u0026quot;按块有序\u0026quot;；即第1块中任一元素的关键字都必须小于第2块中任一元素的关键字；而第2块中任一元素又都必须小于第3块中的任一元素，……。然后使用二分查找及顺序查找。 P类与NP类 通常将可在多项式时间内解决的问题看作“易”解决的问题，将再指数函数时间解决的问题看作“难”的问题。\n不是所有的问题都可以在多项式时间内求解。\n所有可以在多项式时间内求解的判定问题构成P类问题（Polynomial Problem，多项式问题）。\nP类问题是确定性模型下的易解问题类。\nNP类问题是非确定性计算模型下的易验证问题类\n参考1 参考2 P问题、NP问题、NP完全问题和NP难问题 - 知乎 (zhihu.com) 参考3 简析P和NP问题的概念 - 别再闹了 - 博客园 (cnblogs.com) 第二章 递归与分治策略 ","date":"2023-02-22","permalink":"https://gruiaaa.github.io/post/%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/","tags":null,"title":"算法笔记"},{"content":"onlineUserPredict 项目复现 [toc]\n项目说明 项目内容 摘要\n云计算平台以虚拟机为单位按时计算费用，虚拟机数量与在线用户的人数成正比。准确预测在线用户数量，租用合适数量的虚拟机可以节约大量的费用。\n本项目提出了一个基于双层模型的网络游戏在线用户人数预测方法 N-BEATSx-N-HiTS，其特点是利用N-BEATSx 和 NHiTS 为底层预测模型，以随机森林方法进行结果拟合。\n在一个超大规模真实数据集上验证了该方法，该方法单步预测精度达到 97%，多步预测精度也远高于基线方法。根据预测结果租用合适数量的虚拟机，可以节约 40.09%的虚拟机租用成本。该方法能有效处理样本缺失和突发性在线人数剧烈增加的情况。\n工作节点\u0026amp;目录 本地工作节点为10.103.104.117，用户ZSX，密码***。onlineUserPredict 根目录为/home/ZSX/gr/onlineUserPredict\n工作目录\n2023/2/5 更新\n# 2023/2/5 更新\r# 本地工作目录工作目录为\r.\r├── codes\r│ ├── 00_utils\r│ │ ├── Untitled.ipynb\r│ │ └── write2oneCsv.ipynb\r│ ├── 01_joinData\r│ │ └── joinData.ipynb\r│ ├── 02_timeDuration\r│ │ └── timeDuration.ipynb\r│ └── 03_NAOMI\r│ ├── 01missingValuesDetection.ipynb\r│ └── 02anomalyValuesDetection.ipynb\r├── data\r│ ├── 00_gameDataExample\r│ │ ├── fromCXL\r│ │ │ ├── matches_game.csv\r│ │ │ └── private_rooms_game_only1part.csv\r│ │ └── fromHYF\r│ │ ├── clubInfo.csv\r│ │ ├── club_regtime.csv\r│ │ ├── t_game_bf.csv\r│ │ ├── t_game_user_bf.csv\r│ │ ├── userInfo.csv\r│ │ └── user_regtime.csv\r│ ├── 01_joinedData\r│ ├── 02_timeDurationData\r│ │ ├── 15min_TimeDuration.csv\r│ │ ├── 1day_TimeDuration.csv\r│ │ ├── 30min_TimeDuration.csv\r│ │ └── 60min_TimeDuration.csv\r│ └── 03_NAOMI\r│ ├── 1day_timeduration_filledwith0user.csv\r│ ├── anomalyResultFilledWith0_1day_timeduration.csv\r│ └── anomalyResultNoFilled_1day_timeduration.csv\r└── updateCodesFromJupyter.sh\r# HDFS 目录为\rhdfs dfs -du -h /user/GR/OUPD\r20.4 G 61.3 G /user/GR/OUPD/01joinedData\r1.3 M 3.8 M /user/GR/OUPD/15min_TimedurationData\r26.6 K 79.7 K /user/GR/OUPD/1day_TimedurationData\r653.7 K 1.9 M /user/GR/OUPD/30min_TimedurationData\r336.5 K 1009.6 K /user/GR/OUPD/60min_TimedurationData\r2.2 M 6.6 M /user/GR/OUPD/inOneCSV\r7.9 G 23.6 G /user/GR/OUPD/matches_game\r73.1 G 219.4 G /user/GR/OUPD/private_rooms_game\r原项目流程分析 通过分析车所给代码，绘制如下图\n整个项目可分为四个步骤\n对原数据进行join，提取关键属性列（用户在线时间、对局信息等）。 对用户在线时间段进行划分（15min一段、30min一段、1h一段），并进行统计。 使用NAOMI（非自回归多分辨率插补）进行缺失值填充。 完整数据集传入模型，进行训练、预测。 流程复现 00.数据集准备 原始数据集 某游戏平台连续8个月的真实游戏用户数据和日志数据，数据时间从2020年7月至2021年4月。数据包含用户对局过程中的所有静态和动态信息，共计6个表。\n数据集 文件名 描述 详细信息 公会信息表 clubinfo.csv 工会ID、工会成员ID 公会注册表 club_regtime.csv 工会ID、工会注册时间 用户注册表 user_regtime.csv 用户ID、用户注册时间 用户信息表 userInfo.csv 用户ID、用户出生日期、用户性别、注册包ID、上次登录时间、上次登录IP 游戏信息表 t_game_user_bf（存疑） 包房 ID、注册时间、用户生日、性别、公会 ID（存疑） 游戏对局表 t_game_bf.csv 用户 ID、桌号、积分、开局时间、结束时间（部分关键字段） 数据存疑 2023/1/10\n车论文中所写数据集与关键字段如下\n​\t数据集\t关键字段 ​\t公会信息表 用户 ID、公会 ID ​\t公会注册表 公会 ID、注册时间、机器号、游戏包 ID ​\t用户注册表 用户 ID、公会 ID ​\t用户信息表 性别、用户 ID、注册包、最大游戏局数、底金 ​\t游戏信息表 包房 ID、注册时间、用户生日、性别、公会 ID ​\t游戏对局表 用户 ID、桌号、积分、开局时间、结束时间\n但与胡所给原始数据集样本中字段（如上六图）不能完全对应上，主要体现在t_game_user_bf、t_game_bf两表中。\n应该是车or胡对数据做过一些处理（比如join等），车论文中所用是处理后的结果。\n初始数据集无权限获取，所以复现流程在车的数据集基础上进行。\n车\u0026rsquo;s HDFS 数据集 车所给数据集预处理代码为 10.103.104.137:/home/CXL/data_g，从数据预处理代码（GameData.ipynb，Game_Data.ipynb，OnlineUsers2.ipynb）推断，预处理所用到的数据集主要有两个\nhdfs://10.103.104.134:8020/cxl/second\nhdfs://10.103.104.134:8020/cxl/bf_game\n从文件命名格式可看出这两个文件夹是spark处理后的结果。second文件夹下仅一个part，大小为7.9G。bf_game/20200727-20210310文件夹下共616个part，总大小为73.1G。\n进行数据迁移并重命名。在此数据集基础上复现。\n数据集 文件夹名 大小 描述 HDFS Url 对局信息 matches_game 7.9G 每一对局的信息，包含用户ID，用户所属工会ID，起止时间等 hdfs://10.103.104.117:9000\n/user/GR/OUPD/matches_game 包房信息 private_rooms_game 73.1G 每个包房的信息，包含包房的底金，最大对局数、起止时间等 hdfs://10.103.104.117:9000\n/user/GR/OUPD/private_rooms_game 说明：对局信息和包房信息中共同列为对局开始时间、结束时间。（对局要有包房）\n01.joinData joinData.ipynb\n读取hdfs集群中 matches_game/、private_rooms_game/ 下数据，通过用户生日计算年龄，进行join操作（on=\u0026lsquo;btime\u0026rsquo;, how=\u0026lsquo;left\u0026rsquo;），选取特征列，结果如下图\n结果存储在hdfs://10.103.104.117:9000/user/GR/OUPD/01joinedData，共200个part，总大小为20.4G。\n02.timeDuration timeDuration.ipynb\n读取hdfs://10.103.104.117:9000/user/GR/OUPD/01joinedData，进一步选择特征列select(\u0026ldquo;uid\u0026rdquo;,\u0026ldquo;dday\u0026rdquo;,\u0026ldquo;btime\u0026rdquo;,\u0026ldquo;etime\u0026rdquo;,\u0026ldquo;dijin\u0026rdquo;,\u0026ldquo;maxju\u0026rdquo;,\u0026ldquo;cost\u0026rdquo;,\u0026ldquo;age\u0026rdquo;)，将起止时间从时间戳转换为标准时间格式，通过calcalateOnlineTimeDuration(df,timeDuration=30)函数计算用户上线时间所处于哪个时间段，然后通过groupby和count，计算出每个时间段在线人数以及相应特征列的平均值。结果如下图所示\n结果按时间段间隔命名，存储在HDFS上\n时间段间隔 结果路径 大小 15minn /user/GR/OUPD/15min_TimedurationData 1.3 M 30min /user/GR/OUPD/30min_TimedurationData 653.8 K 60min /user/GR/OUPD/60min_TimedurationData 336.4 K 1day /user/GR/OUPD/1day_TimedurationData 26.6 K 此后项目在此数据集上进行。\n03.NAOMI 缺失值检测 目的：在1day_TimedurationData基础上检测数据集有无日期缺失。\n01missingValuesDetection.ipynb\n读取数据后通过max、min获取日期的起止时间，得到跨度天数，通过date_range生成起止时间序列，与数据做差集，即可得到数据集中缺失天数。如下图所示\n将缺失值填充为0，画折线图即可直观看到缺失情况，如下图所示，填充结果存储为03_NAOMI/1day_timeduration_filledwith0user.csv\n异常值检测 异常检测（Anomaly detection）是定义是从正常的时间序列中识别不正常的事件或行为的过程。\n常用的异常检测算法可大致分为以下几种（来自网络，参考1 参考2）\n统计方法\n最常用的方法就是基于k-sigma的同比算法，这是一个快速而且有效的算法。简单来说，即当前数据点的状态由之前不同周期的相同位置数据(比如上周或者前一天的同一时刻)决定，通过历史同期的数据分布来确定当前数据的合理波动范围。它的初始假设，是局部数据符合正态分布。这个算法有效利用了历史同期的数据，避免了全局使用唯一的固定阈值来衡量是否异常，同时还具有算法计算快速、原理易懂可解释的优点。\n预测方法\n预测方法是异常检测中最常用的方法，基本思路是通过比较预测值和真实值的差异，判定是否异常。它包括传统的时序预测模型ARIMA、渐进梯度回归树GBRT、长短时记忆网络LSTM以及Facebook开源算法Prophet等等\n有监督学习\n有监督的算法有很多，如基于树模型的随机森林、lightGBM，神经网络MLP等等，其整体思路是提取各种各样的统计特征（如前几个数据点的原始值，最近一段时间的均值、标准差、偏度等等），直接丢给模型去训练，算法会根据标注自动选择最有效的特征用以建模。有监督算法往往可以获得更高的算法准确度，但缺点也是十分明显的——最大的问题就是，我们需要大量的人工标注，覆盖全面的数据类型和异常情况，而这在实际场景中是极难实现的。实际生产中，我们极少考虑这类算法，除非异常场景很明确且历史中存在多次相似的情况。\n深度学习检测\n最近几年，通过深度学习生成模型来做异常检测的算法越来越多，效果甚至可以超过一般的有监督学习方法。主要可以分为以下这么几种\na) 针对正常数据进行训练建模，然后通过高重构误差来识别异常点，即生成式（Generative）的算法，往往是无监督的，如自编码器（Auto Encoder）类或者回声状态网络（Echo State Networks）。生成模型的优势就是算法准确率高、极少人工干预，但单纯的算法仍存在一些不足。如需要长时间表现稳定的历史数据，需要较长的训练时间，且同样会面临衡量差值大小与异常的关系这类问题等等。\nb) 对数据的概率分布进行建模，然后根据样本点与极低概率的关联性来识别异常点，如DAGMM\nc) 通过标注数据，告诉模型正常数据点长什么样，异常数据点长什么样，然后通过有监督算法训练分类模型，也称判别式（Discriminative）算法。\n经过对异常值检测算法的简单了解，我在本项目中决定选用统计方法对异常值进行检测，原因是统计方法可自定义规则，计算快速，原理通俗易懂。\n统计方法中常见的方法如箱型图、3σ方法、分位点法、离群点检测都是基于非时序数据的方法，即它们所检测的是在一个整体数据集的情况下，所有数据偏离整体分布的情况，偏离一定程度即视为异常值。所以以上方法不适用于时序数据的检测。\n定义规则：第N个数据的值处于前7个数据均值上下15%之间时，视为正常值，否则为异常值\n以1day_timeduration_filledwith0user.csv为例，进行异常值检测\n0填充异常值检测 运行结果如下图\n其中-7dayMean列为前7次数据的平均值，percent为该次数据在前7天平均值上的涨跌比率，anomaly为是否是异常值（超过上下15%）。 -折线图中标出异常值- -涨跌比率随时间变化- 从图中结果分析，使用在0填充后的数据集上使用前N天均值检测异常值会出现0填充影响正常数据的情况（因为0拉低了平均值），所以使用未进行0填充的数据再进行一遍检测，且使用散点图绘图。\n无填充异常值检测 -异常值结果- -在线人数散点图- -异常值标注- 检测结果分别存储为reslutFilledWith0.csv,reslutNoFilled.csv\n两个结果对比，取交集如下图，只有一个共同值。\n缺失值填充 在1day_TimeDuration.csv上通过缺失值检测可知缺少3天数据集，所以要进行缺失值填充。\n填充方法选取NAOMI(非自回归多分辨率插补)进行填充，该方法为论文所提出。\n简要读完论文及代码、查看之前代码的运行记录后，决定在复现中先跳过此步骤，使用之前的填充好的数据集，原因如下：\n填充一个属性列就需要13h+的时间来训练模型，一共需要填充五个，耗时久 NAOMI官方代码修改较麻烦，魔改耗时久 使用torch自建模型不太了解，需要花时间学习 综上，以实现在线人数预测为目的，重点放在在线实现，所以可降低数据缺失值填充重要性，留待之后实现。\n之后使用车所给rawdata文件夹中的填充好的rawdata_15min.csv\n04.N-BEATSx-N-HiTS 2023.02.08 更新\nGPU服务器连接不上，只好搁置，等到校恢复正常后再进行。\n","date":"2023-01-16","permalink":"https://gruiaaa.github.io/post/onlineuserpredict/","tags":null,"title":"OnlineUserPredict"}]